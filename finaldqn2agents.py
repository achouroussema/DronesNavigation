# -*- coding: utf-8 -*-
"""FinalDQN2Agents.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MLWasjASAeP053bnhvkHw1voC9IpD8o1
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque

# Define the environment
class Environment:
    def __init__(self, num_agents):
        self.num_agents = num_agents
        self.state = None  # Initialize the state attribute

    def reset(self):
        self.state = [[0, 0]] * self.num_agents  # Reset the state to initial positions
        return self.state
    
    def step(self, actions):
        next_state = []
        rewards = []
        done = False
        # Update the state based on the actions
        for agent_id, action in enumerate(actions):
            state = self.state[agent_id]
            if action == 0:  # Up
                next_state.append([state[0] - 1, state[1]])
            elif action == 1:  # Down
                next_state.append([state[0] + 1, state[1]])
            elif action == 2:  # Left
                next_state.append([state[0], state[1] - 1])
            elif action == 3:  # Right
                next_state.append([state[0], state[1] + 1])

            # Calculate the reward based on the next state
            agent_reward = 0
            if next_state[agent_id] == [3, 4]:  # IOT Device
                agent_reward = 25
                done = True
                print("Data transmitted")
            elif next_state[agent_id] == [7, 2]:  # Obstacle
                agent_reward = -5
            elif next_state[agent_id] == [5, 3]:  # NFZ
                agent_reward = -10
            else:
                agent_reward = -1

            rewards.append(agent_reward)

        self.state = next_state
        return next_state, rewards, done, {}


# Define the Q-network
class QNetwork(nn.Module):
    def __init__(self, input_size, output_size):
        super(QNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size, 32)
        self.fc2 = nn.Linear(32, 32)
        self.fc3 = nn.Linear(32, output_size)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# Define the agent
class DQNAgent:
    def __init__(self, input_size, output_size, device):
        self.device = device
        self.input_size = input_size
        self.output_size = output_size
        self.q_network = QNetwork(input_size, output_size).to(device)
        self.target_network = QNetwork(input_size, output_size).to(device)
        self.target_network.load_state_dict(self.q_network.state_dict())
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.001)
        self.memory = deque(maxlen=10000)
        self.gamma = 0.99
        self.epsilon = 1.0
        self.epsilon_decay = 0.995
        self.epsilon_min = 0.01

    def select_action(self, state, agent_id):
        if np.random.rand() <= self.epsilon:
            action = np.random.randint(self.output_size)
        else:
            with torch.no_grad():
                q_values = self.q_network(state)
                action = torch.argmax(q_values).item()
        return action

    def store_transition(self, state, action, reward, next_state, done, agent_id):
        self.memory.append((state, action, reward, next_state, done, agent_id))

    def update_q_values(self, agent_id):
        batch_size = 32
        if len(self.memory) < batch_size:
            return

        transitions = np.random.choice(len(self.memory), batch_size, replace=False)
        batch = [self.memory[i] for i in transitions]
        state_batch = torch.tensor([transition[0] for transition in batch], dtype=torch.float32).to(self.device)
        action_batch = torch.tensor([[transition[1]] for transition in batch], dtype=torch.long).to(self.device)
        reward_batch = torch.tensor([transition[2] for transition in batch], dtype=torch.float32).to(self.device)
        next_state_batch = torch.tensor([transition[3] for transition in batch], dtype=torch.float32).to(self.device)
        done_batch = torch.tensor([transition[4] for transition in batch], dtype=torch.float32).to(self.device)

        q_values = self.q_network(state_batch)
        next_q_values = self.target_network(next_state_batch)

        q_values_selected = torch.gather(q_values, 1, action_batch)
        max_next_q_values = torch.max(next_q_values, dim=1)[0]
        expected_q_values = reward_batch + self.gamma * max_next_q_values * (1 - done_batch)

        loss = nn.MSELoss()(q_values_selected, expected_q_values.unsqueeze(1))
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

    def update_target_network(self):
        self.target_network.load_state_dict(self.q_network.state_dict())

    def epsilon_decay_step(self):
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

# Create the environment and agents
num_agents = 2
env = Environment(num_agents)
agents = [DQNAgent(input_size=2, output_size=4, device=torch.device("cuda" if torch.cuda.is_available() else "cpu")) for _ in range(num_agents)]

# Training loop
num_episodes = 15000
max_steps = 100
total_rewards = [0] * env.num_agents
episode_rewards = []
epsilon_list = []


charge_percentages = [100.0] * env.num_agents  # Initialize charge percentages

for episode in range(num_episodes):
    # Reset the environment
    env.reset()

    # Get the initial states
    states = env.reset()

    done = False
    step = 0

    while not done and step < max_steps:
        actions = []
        for agent_id, agent in enumerate(agents):
            state = states[agent_id]
            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(agent.device)
            action = agent.select_action(state_tensor, agent_id)
            actions.append(action)

        next_state, rewards, done, _ = env.step(actions)

        for agent_id, agent in enumerate(agents):
            agent.store_transition(states[agent_id], actions[agent_id], rewards[agent_id], next_state[agent_id], done, agent_id=agent_id)
            agent.update_q_values(agent_id)
            total_rewards[agent_id] += rewards[agent_id]
            epsilon_list.append(epsilon)

        states = next_state
        step += 1

        # Check if any agent has reached the IoT device
        if [3, 4] in states:
            done = True

        # Update charge percentages every 2 steps
        if step % 2 == 0:
            for agent_id in range(env.num_agents):
                if charge_percentages[agent_id] > 50.0:
                    charge_percentages[agent_id] -= 1.0

       

    episode_rewards.append(total_rewards.copy())  # Store the total rewards for the episode



    # Print episode information
    print(f"Episode {episode + 1}:  Total rewards for this episode = {total_rewards}, Charge percentages = {charge_percentages}")

import matplotlib.pyplot as plt
import numpy as np

# Plot the rewards for each agent
for agent_id, rewards in enumerate(zip(*episode_rewards)):
    plt.plot(np.arange(1, num_episodes + 1), rewards, label=f'Agent {agent_id+1}')

plt.xlabel('Episode')
plt.ylabel('Total Rewards')
plt.title('Rewards during Training')
plt.legend()
plt.show()

