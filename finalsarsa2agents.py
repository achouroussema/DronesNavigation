# -*- coding: utf-8 -*-
"""FinalSARSA2Agents.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uGTr9-6yRhZyS933Hm1eX3zqrJpsoz_G
"""

import numpy as np
import random
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from multiprocessing.pool import ThreadPool

alpha = 0.001 #learning_rate
gamma = 0.9 #discount_factor
epsilon = 1.0 #exploratin_rate
epsilon_min = 0.001
epsilon_decay = 0.99
episodes = 10000
num_agents = 2

obstacle1 = 16
obstacle2 = 88
NFZ = 21
iot = 43

env_rows = 10
env_cols = 10
state_count = env_rows * env_cols

agent_start_positions = [(0, 0), (0, 1)]

q_tables = [np.zeros((state_count, 4)) for _ in range(num_agents)]

def get_next_action(agent_id, state):
    global epsilon
    if random.uniform(0, 1) < epsilon:
        return random.randint(0, 3)
    q_values = q_tables[agent_id][state]
    max_q_value = np.max(q_values)
    actions_with_max_q_value = np.where(q_values == max_q_value)[0]
    if actions_with_max_q_value.size > 0:
        action = np.random.choice(actions_with_max_q_value)
    else:
        action = random.randint(0, 3)
    return action

def update_q_table(agent_id, state, action, reward, next_state, next_action):
    old_value = q_tables[agent_id][state, action]
    next_q_value = q_tables[agent_id][next_state, next_action]
    new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_q_value)
    q_tables[agent_id][state, action] = new_value

def drone_action(agent, action):
    global states, charge_levels, step, data_transmitted
    if action == 0:
        if states[agent] % env_cols < env_cols - 1:
            next_state = states[agent] + 1
        else:
            next_state = states[agent]
    elif action == 1:
        if states[agent] % env_cols > 0:
            next_state = states[agent] - 1
        else:
            next_state = states[agent]
    elif action == 2:
        if states[agent] >= env_cols:
            next_state = states[agent] - env_cols
        else:
            next_state = states[agent]
    elif action == 3:
        if states[agent] < state_count - env_cols:
            next_state = states[agent] + env_cols
        else:
            next_state = states[agent]


    reward = 0
    if next_state == iot:
        if random.uniform(0, 1) < data_transmission_prob:
            if not data_transmitted:
                print("Data Transmitted!")
                reward = 25
                data_transmitted = True
            else:
                reward = 10  # Modified reward for reaching IoT device without data transmission

    elif next_state == obstacle1 or next_state == obstacle2:
        reward = -5
        
    elif next_state == NFZ:
        reward = -10
    else:
        reward = -1

    next_action = get_next_action(agent, next_state)

    update_q_table(agent, states[agent], action, reward, next_state, next_action)

    states[agent] = next_state

    if (step + 1) % 2 == 0:
        charge_levels[agent] -= 1

    return next_state, reward

def run_episode(num_agents, max_steps):
    global states, charge_levels, step, data_transmitted
    episode_rewards = [0] * num_agents
    step = 0

    data_transmitted = False

    while any(s != iot for s in states) and step < max_steps:
        pool = ThreadPool(processes=num_agents)
        results = pool.starmap(drone_action, [(agent, get_next_action(agent, states[agent])) for agent in range(num_agents)])
        pool.close()
        pool.join()

        for agent, (next_state, reward) in enumerate(results):
            episode_rewards[agent] += reward

        step += 1

    global epsilon
    epsilon = max(epsilon_min, epsilon * epsilon_decay)

    charge_percentages = [charge_level / 100 * 100 for charge_level in charge_levels]

    return episode_rewards, charge_percentages

total_rewards = []
charge_percentages_list = []
episodes_list = []
epsilon_list = []


max_steps_per_episode = 100

data_transmission_prob = 0.8 

for i in range(episodes):
    states = [agent_start_positions[agent][0] * env_cols + agent_start_positions[agent][1] for agent in range(num_agents)]
    charge_levels = [100] * num_agents

    total_reward, charge_percentages = run_episode(num_agents, max_steps_per_episode)
    total_rewards.append(total_reward)
    charge_percentages_list.append(charge_percentages)
    episodes_list.append(i + 1)
    epsilon_list.append(epsilon)
    print(f"Episode {i + 1}: Epsilon = {epsilon:.2f}, Alpha = {alpha:.2f}, "
          f"Total rewards for this episode = {total_reward}, "
          f"Charge percentages = {charge_percentages}")

for drone in range(num_agents):
    print(f"Final Q-table for Drone {drone+1}:")
    print(q_tables[drone])
    print()

# Plot epsilon values over episodes
plt.plot(episodes_list, epsilon_list)
plt.xlabel('Episode')
plt.ylabel('Epsilon')
plt.title('Epsilon Decay')
plt.show()

# Plot total rewards per episode
plt.plot(episodes_list, total_rewards)
plt.xlabel("Episode")
plt.ylabel("Total Reward")
plt.title("Total Reward per Episode for the 2 UAVs")
plt.show()

# Visualization code
fig, axs = plt.subplots(num_agents, 1, figsize=(8, 6))

# Plot charge percentages for each episode and agent
for agent in range(num_agents):
    axs[agent].plot(episodes_list, [charge_percentages[agent] for charge_percentages in charge_percentages_list])
    axs[agent].set_xlabel('Episode')
    axs[agent].set_ylabel('Charge Percentage')
    axs[agent].set_title(f'Charge Percentage for UAV : Unmanned aerial vehicle {agent + 1}')

# Adjust spacing between subplots
plt.tight_layout()

# Show the plot
plt.show()

# Flatten the total_rewards list
flattened_rewards = [reward for episode_rewards in total_rewards for reward in episode_rewards]
flattened_charge_percentages = [charge_percentages for episode_charge_percentages in charge_percentages_list for charge_percentages in episode_charge_percentages]

# Calculate moving average of total rewards
window_size = 100
moving_avg_rewards = np.convolve(flattened_rewards, np.ones(window_size)/window_size, mode='valid')

# Calculate moving average of charge percentages
moving_avg_charge_percentages = np.convolve(flattened_charge_percentages, np.ones(window_size)/window_size, mode='valid')

# Plot the moving average rewards per episode
plt.plot(episodes_list[window_size - 1:], moving_avg_rewards[:len(episodes_list[window_size - 1:])])
plt.xlabel('Episode')
plt.ylabel('Moving Average Reward')
plt.title(f'Moving Average Reward (Window Size = {window_size})')
plt.show()

# Plot the moving average charge percentages per episode
plt.plot(episodes_list[window_size - 1:], moving_avg_charge_percentages[:len(episodes_list[window_size - 1:])])
plt.xlabel('Episode')
plt.ylabel('Moving Average Charge Percentage')
plt.title(f'Moving Average Charge Percentage (Window Size = {window_size})')
plt.show()

