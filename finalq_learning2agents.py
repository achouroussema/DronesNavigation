# -*- coding: utf-8 -*-
"""FinalQ-Learning2Agents.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E43bK1u2utLUTYtzTvZjI3ytWfPrVL7P
"""

import numpy as np
import random
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from multiprocessing.pool import ThreadPool



# Set hyperparameters
alpha = 0.001 #learning_rate
gamma = 0.9  #discount_factor
epsilon = 1.0 #exploration_rate
epsilon_min = 0.001
epsilon_decay = 0.99
episodes = 10000
num_agents = 2

# Initialize obstacles and IoT device
obstacle1 = 16
obstacle2 = 88
NFZ = 21
iot = 43

# Initialize environment
env_rows = 10
env_cols = 10
state_count = env_rows * env_cols

# Initialize starting position of the agents
agent_start_positions = [(0, 0), (0, 1)]

# Initialize Q-tables for each agent
q_tables = [np.zeros((state_count, 4)) for _ in range(num_agents)]

# Define function for getting the next action for a given agent
def get_next_action(agent_id, state):
    global epsilon
    # Take random action with probability epsilon
    if random.uniform(0, 1) < epsilon:
        return random.randint(0, 3)
    # Otherwise, choose action with the highest Q value
    q_values = q_tables[agent_id][state]
    max_q_value = np.max(q_values)
    actions_with_max_q_value = np.where(q_values == max_q_value)[0]

    # Choose action with the highest Q value among non-obstacle actions
    if actions_with_max_q_value.size > 0:
        action = np.random.choice(actions_with_max_q_value)
    else:
        action = random.randint(0, 3)
    return action

# Define function for updating Q-table for a given agent
def update_q_table(agent_id, state, action, reward, next_state):
    old_value = q_tables[agent_id][state, action]
    next_max = np.max(q_tables[agent_id][next_state])
    new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)
    q_tables[agent_id][state, action] = new_value

def drone_action(agent, action):
    global states, charge_levels, step, data_transmitted
    if action == 0:  # Move right
        if states[agent] % env_cols < env_cols - 1:
            next_state = states[agent] + 1
        else:
            next_state = states[agent]
    elif action == 1:  # Move left
        if states[agent] % env_cols > 0:
            next_state = states[agent] - 1
        else:
            next_state = states[agent]
    elif action == 2:  # Move up
        if states[agent] >= env_cols:
            next_state = states[agent] - env_cols
        else:
            next_state = states[agent]
    elif action == 3:  # Move down
        if states[agent] < state_count - env_cols:
            next_state = states[agent] + env_cols
        else:
            next_state = states[agent]

    # Update Q-table with new Q-value
    reward = 0
    if next_state == iot:
          if random.uniform(0, 1) < data_transmission_prob:
              if not data_transmitted:
                  print("Data Transmitted!")
                  reward = 15
                  data_transmitted = True
              else:
                  reward = 10
    elif next_state == obstacle1 or next_state == obstacle2:
        reward = -5
    elif next_state == NFZ:
        reward = -10
    else:
        reward = -1
    update_q_table(agent, states[agent], action, reward, next_state)

    # Update state and total reward for the agent
    states[agent] = next_state

    # Decrease charge level every 4 actions
    if (step + 1) % 2 == 0:
        charge_levels[agent] -= 1

    return next_state, reward

def run_episode(num_agents, max_steps):
    global states, charge_levels, step, data_transmitted
    # Initialize total rewards for the episode
    episode_rewards = [0] * num_agents
    # Initialize step counter
    step = 0

    data_transmitted = False

    # Play until any agent reaches the IoT device or maximum steps reached
    while any(s != iot for s in states) and step < max_steps:
        # Create a process pool
        pool = ThreadPool(processes=num_agents)
        # Execute drone actions in parallel
        results = pool.starmap(drone_action, [(agent, get_next_action(agent, states[agent])) for agent in range(num_agents)])
        pool.close()
        pool.join()

        # Accumulate rewards for each agent
        for agent, (next_state, reward) in enumerate(results):
            episode_rewards[agent] += reward

        # Increment step counter
        step += 1

    # Update epsilon for next episode
    global epsilon
    epsilon = max(epsilon_min, epsilon * epsilon_decay)

    # Calculate percentage of charge for each drone
    charge_percentages = [charge_level / 100 * 100 for charge_level in charge_levels]

    # Return the total rewards and charge percentages for this episode
    return episode_rewards, charge_percentages

# Initialize lists to store rewards and episodes
total_rewards = []
charge_percentages_list = []
episodes_list = []
epsilon_list = []

# Set the maximum number of steps per episode
max_steps_per_episode = 100

data_transmission_prob = 0.8 

# Run multiple episodes
for i in range(episodes):
    # Initialize states and charge levels for each agent
    states = [agent_start_positions[agent][0] * env_cols + agent_start_positions[agent][1] for agent in range(num_agents)]
    charge_levels = [100] * num_agents

    total_reward, charge_percentages = run_episode(num_agents, max_steps_per_episode)
    total_rewards.append(total_reward)
    charge_percentages_list.append(charge_percentages)
    episodes_list.append(i + 1)
    epsilon_list.append(epsilon)
    print(f"Episode {i + 1}: Epsilon = {epsilon:.2f}, Alpha = {alpha:.2f}, "
          f"Total rewards for this episode = {total_reward}, "
          f"Charge percentages = {charge_percentages}")

# Print the final Q-table for each drone
for drone in range(num_agents):
    print(f"Final Q-table for Drone {drone+1}:")
    print(q_tables[drone])
    print()

# Plot epsilon values over episodes
plt.plot(episodes_list, epsilon_list)
plt.xlabel('Episode')
plt.ylabel('Epsilon')
plt.title('Epsilon Decay')
plt.show()

# Plot total rewards per episode
plt.plot(episodes_list, total_rewards)
plt.xlabel("Episode")
plt.ylabel("Total Reward")
plt.title("Total Reward per Episode for the 2 UAVs")
plt.show()

# Visualization code
fig, axs = plt.subplots(num_agents, 1, figsize=(8, 6))

# Plot charge percentages for each episode and agent
for agent in range(num_agents):
    axs[agent].plot(episodes_list, [charge_percentages[agent] for charge_percentages in charge_percentages_list])
    axs[agent].set_xlabel('Episode')
    axs[agent].set_ylabel('Charge Percentage')
    axs[agent].set_title(f'Charge Percentage for UAV : Unmanned aerial vehicle {agent + 1}')

# Adjust spacing between subplots
plt.tight_layout()

# Show the plot
plt.show()

# Flatten the total_rewards list
flattened_rewards = [reward for episode_rewards in total_rewards for reward in episode_rewards]
flattened_charge_percentages = [charge_percentages for episode_charge_percentages in charge_percentages_list for charge_percentages in episode_charge_percentages]

# Calculate moving average of total rewards
window_size = 100
moving_avg_rewards = np.convolve(flattened_rewards, np.ones(window_size)/window_size, mode='valid')

# Calculate moving average of charge percentages
moving_avg_charge_percentages = np.convolve(flattened_charge_percentages, np.ones(window_size)/window_size, mode='valid')

# Plot the moving average rewards per episode
plt.plot(episodes_list[window_size - 1:], moving_avg_rewards[:len(episodes_list[window_size - 1:])])
plt.xlabel('Episode')
plt.ylabel('Moving Average Reward')
plt.title(f'Moving Average Reward (Window Size = {window_size})')
plt.show()

# Plot the moving average charge percentages per episode
plt.plot(episodes_list[window_size - 1:], moving_avg_charge_percentages[:len(episodes_list[window_size - 1:])])
plt.xlabel('Episode')
plt.ylabel('Moving Average Charge Percentage')
plt.title(f'Moving Average Charge Percentage (Window Size = {window_size})')
plt.show()

